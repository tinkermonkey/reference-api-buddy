# Technical Design Document: HTTP Caching Proxy Module

## 1. Executive Summary

This document outlines the technical design for a Python module that provides HTTP caching proxy functionality with cryptographic access control. The module is designed to be embedded within parent applications for development environments, offering intelligent caching, progressive throttling, and detailed metrics to reduce API costs and prevent rate limiting.

## 2. System Architecture

### 2.1 High-Level Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  Parent App     │    │  Caching Proxy  │    │  External APIs  │
│                 │    │     Module      │    │                 │
│ ┌─────────────┐ │    │ ┌─────────────┐ │    │ ┌─────────────┐ │
│ │ HTTP Client │─┼────┼─│HTTP Server  │ │    │ │ api.openai  │ │
│ │ Libraries   │ │    │ │             │ │    │ │ .com        │ │
│ └─────────────┘ │    │ └─────────────┘ │    │ └─────────────┘ │
│                 │    │        │        │    │                 │
│ ┌─────────────┐ │    │ ┌─────────────┐ │    │ ┌─────────────┐ │
│ │Configuration│ │    │ │Cache Engine │ │    │ │ Other APIs  │ │
│ │   Manager   │─┼────┼─│             │ │    │ │             │ │
│ └─────────────┘ │    │ └─────────────┘ │    │ └─────────────┘ │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

### 2.2 Module Components

#### 2.2.1 Core Components
- **CachingProxy**: Main module class and entry point
- **HTTPServerHandler**: Request processing and routing
- **CacheEngine**: Cache storage and retrieval logic
- **SecurityManager**: Cryptographic key management and validation
- **ThrottleManager**: Rate limiting and progressive throttling
- **MetricsCollector**: Performance and usage tracking
- **ConfigurationManager**: Runtime configuration management

#### 2.2.2 Supporting Components
- **DatabaseManager**: SQLite database operations
- **URLTransformer**: Domain mapping and URL transformation
- **ResponseProcessor**: Response caching and serving
- **LoggingAdapter**: Integration with parent application logging

## 3. Detailed Component Design

### 3.1 CachingProxy (Main Class)

```python
class CachingProxy:
    """Main entry point for the caching proxy module."""

    def __init__(self, config: dict):
        """Initialize the proxy with configuration."""
        self.config = self._validate_and_merge_config(config)
        self.secure_key = None
        self.server = None
        self.running = False
        self.start_time = None

        # Initialize components
        self.security_manager = SecurityManager(self.config.get('security', {}))
        self.cache_engine = CacheEngine(self.config.get('cache', {}))
        self.throttle_manager = ThrottleManager(self.config.get('throttling', {}))
        self.metrics_collector = MetricsCollector()
        self.config_manager = ConfigurationManager(self.config)

        # Generate secure key if security enabled
        if self.config.get('security', {}).get('require_secure_key', False):
            self.secure_key = self.security_manager.generate_secure_key()

    def start(self, blocking: bool = False) -> None:
        """Start the proxy server."""

    def stop(self) -> None:
        """Stop the proxy server and cleanup resources."""

    def get_secure_key(self) -> str | None:
        """Return the secure key if security is enabled."""

    def is_running(self) -> bool:
        """Check if the proxy server is running."""

    def get_metrics(self) -> dict:
        """Return current metrics and statistics."""

    def clear_cache(self, domain: str = None) -> int:
        """Clear cache entries for a domain or all domains."""

    def update_config(self, key_path: str, value: any) -> None:
        """Update configuration at runtime."""

    def reload_config(self, new_config: dict) -> None:
        """Reload configuration completely."""
```

### 3.2 HTTPServerHandler

```python
class ProxyHTTPRequestHandler(BaseHTTPRequestHandler):
    """HTTP request handler for the caching proxy."""

    def __init__(self, proxy_instance: CachingProxy):
        self.proxy = proxy_instance
        self.security_manager = proxy_instance.security_manager
        self.cache_engine = proxy_instance.cache_engine
        self.throttle_manager = proxy_instance.throttle_manager
        self.metrics_collector = proxy_instance.metrics_collector

    def do_GET(self):
        """Handle GET requests."""
        self._handle_request('GET')

    def do_POST(self):
        """Handle POST requests."""
        self._handle_request('POST')

    def do_PUT(self):
        """Handle PUT requests (no caching)."""
        self._handle_request('PUT')

    def do_DELETE(self):
        """Handle DELETE requests (no caching)."""
        self._handle_request('DELETE')

    def _handle_request(self, method: str):
        """Core request handling logic."""
        try:
            # 1. Extract and validate secure key
            extracted_key, cleaned_path = self.security_manager.extract_secure_key(
                self.path, self.headers, self._parse_query_params()
            )

            if not self.security_manager.validate_request(extracted_key):
                self._send_error_response(403, "Forbidden")
                return

            # 2. Check if this is an admin endpoint
            if cleaned_path.startswith('/admin/'):
                self._handle_admin_request(cleaned_path, method)
                return

            # 3. Transform URL and check domain mapping
            target_url = self._transform_url(cleaned_path)
            if not target_url:
                self._send_error_response(404, "Domain mapping not found")
                return

            # 4. Check throttling
            domain = self._extract_domain(target_url)
            if self.throttle_manager.should_throttle(domain):
                delay = self.throttle_manager.get_throttle_delay(domain)
                self._send_error_response(429, f"Rate limited. Retry after {delay}s")
                return

            # 5. Check cache for cacheable methods
            if method in ['GET', 'POST']:
                cached_response = self._check_cache(method, target_url)
                if cached_response:
                    self._serve_cached_response(cached_response)
                    self.metrics_collector.record_cache_hit(domain)
                    return

            # 6. Forward to upstream
            upstream_response = self._forward_to_upstream(method, target_url)

            # 7. Cache response if appropriate
            if method in ['GET', 'POST'] and self._should_cache_response(upstream_response):
                self._store_in_cache(method, target_url, upstream_response)

            # 8. Send response to client
            self._send_upstream_response(upstream_response)
            self.metrics_collector.record_cache_miss(domain)

        except Exception as e:
            self._handle_request_error(e)
```

### 3.3 SecurityManager

```python
class SecurityManager:
    """Handles cryptographic key generation and validation."""

    def __init__(self, security_config: dict):
        self.config = security_config
        self.secure_key = None
        self.security_enabled = security_config.get('require_secure_key', False)
        self.key_position = security_config.get('key_position', 'path_prefix')
        self.admin_rate_limiter = RateLimiter(
            security_config.get('admin_rate_limit_per_minute', 10)
        )

    def generate_secure_key(self) -> str:
        """Generate a cryptographically secure key."""
        import secrets
        import base64

        key_bytes = secrets.token_bytes(32)
        self.secure_key = base64.urlsafe_b64encode(key_bytes).decode('ascii').rstrip('=')
        return self.secure_key

    def extract_secure_key(self, request_path: str, headers: dict,
                          query_params: dict) -> tuple[str|None, str]:
        """Extract secure key from request and return cleaned path."""
        if not self.security_enabled:
            return None, request_path

        if self.key_position == 'path_prefix':
            return self._extract_from_path(request_path)
        elif self.key_position == 'query_param':
            return self._extract_from_query(request_path, query_params)
        elif self.key_position == 'header':
            return self._extract_from_header(request_path, headers)

        return None, request_path

    def validate_request(self, provided_key: str) -> bool:
        """Validate the provided key with constant-time comparison."""
        if not self.security_enabled:
            return True

        if not self.secure_key or not provided_key:
            return False

        import hmac
        return hmac.compare_digest(
            self.secure_key.encode('utf-8'),
            provided_key.encode('utf-8')
        )

    def _extract_from_path(self, request_path: str) -> tuple[str|None, str]:
        """Extract key from path prefix: /{key}/domain.com/path"""
        parts = request_path.strip('/').split('/')
        if len(parts) >= 2:
            potential_key = parts[0]
            cleaned_path = '/' + '/'.join(parts[1:])
            return potential_key, cleaned_path
        return None, request_path

    def _extract_from_query(self, request_path: str,
                           query_params: dict) -> tuple[str|None, str]:
        """Extract key from query parameter."""
        param_name = self.config.get('key_param_name', 'proxy_key')
        key = query_params.get(param_name)

        if key:
            # Remove key from query params and rebuild path
            filtered_params = {k: v for k, v in query_params.items() if k != param_name}
            if filtered_params:
                from urllib.parse import urlencode
                query_string = urlencode(filtered_params)
                path_without_query = request_path.split('?')[0]
                cleaned_path = f"{path_without_query}?{query_string}"
            else:
                cleaned_path = request_path.split('?')[0]
            return key, cleaned_path

        return None, request_path

    def _extract_from_header(self, request_path: str,
                            headers: dict) -> tuple[str|None, str]:
        """Extract key from HTTP header."""
        header_name = self.config.get('key_header_name', 'X-Proxy-Key')
        key = headers.get(header_name)
        return key, request_path
```

### 3.4 CacheEngine

```python
class CacheEngine:
    """Handles cache storage, retrieval, and management."""

    def __init__(self, cache_config: dict):
        self.config = cache_config
        self.database_path = cache_config.get('database_path', ':memory:')
        self.default_ttl_days = cache_config.get('default_ttl_days', 7)
        self.max_entries = cache_config.get('max_entries', 10000)
        self.enable_compression = cache_config.get('enable_compression', True)

        # Initialize database
        self.db_manager = DatabaseManager(self.database_path)
        self._setup_database_schema()
        self._cleanup_expired_entries()

    def get_cached_response(self, cache_key: str) -> CachedResponse | None:
        """Retrieve cached response if valid and not expired."""
        try:
            result = self.db_manager.execute_query(
                "SELECT response_data, headers, status_code, created_at, ttl_seconds "
                "FROM cache_entries WHERE cache_key = ? AND "
                "datetime('now') < datetime(created_at, '+' || ttl_seconds || ' seconds')",
                (cache_key,)
            )

            if result:
                response_data, headers, status_code, created_at, ttl_seconds = result[0]

                # Decompress if needed
                if self.enable_compression:
                    response_data = self._decompress_data(response_data)

                return CachedResponse(
                    data=response_data,
                    headers=json.loads(headers),
                    status_code=status_code,
                    created_at=created_at,
                    ttl_seconds=ttl_seconds
                )

        except Exception as e:
            # Log error but don't fail the request
            self._log_cache_error("get", cache_key, e)

        return None

    def store_response(self, cache_key: str, response: HTTPResponse,
                      ttl_seconds: int = None) -> bool:
        """Store response in cache with optional TTL override."""
        try:
            if ttl_seconds is None:
                ttl_seconds = self.default_ttl_days * 24 * 3600

            response_data = response.data

            # Compress if enabled and data is large enough
            if self.enable_compression and len(response_data) > 1024:
                response_data = self._compress_data(response_data)

            # Check cache size limits
            if not self._check_storage_limits(len(response_data)):
                self._cleanup_cache_entries()

            self.db_manager.execute_update(
                "INSERT OR REPLACE INTO cache_entries "
                "(cache_key, response_data, headers, status_code, created_at, ttl_seconds) "
                "VALUES (?, ?, ?, ?, datetime('now'), ?)",
                (cache_key, response_data, json.dumps(dict(response.headers)),
                 response.status_code, ttl_seconds)
            )

            return True

        except Exception as e:
            self._log_cache_error("store", cache_key, e)
            return False

    def generate_cache_key(self, method: str, url: str, body: bytes = None,
                          content_type: str = None) -> str:
        """Generate cache key based on method, URL, and body."""
        import hashlib

        # Normalize URL
        normalized_url = self._normalize_url(url)

        # Create base key with method and URL
        key_components = [method.upper(), normalized_url]

        # Add body for POST requests
        if method.upper() == 'POST' and body:
            normalized_body = self._normalize_request_body(body, content_type)
            key_components.append(normalized_body)

        # Generate SHA256 hash
        key_string = ':'.join(key_components)
        return hashlib.sha256(key_string.encode('utf-8')).hexdigest()

    def clear_cache(self, domain: str = None) -> int:
        """Clear cache entries for a domain or all entries."""
        if domain:
            result = self.db_manager.execute_update(
                "DELETE FROM cache_entries WHERE cache_key LIKE ?",
                (f"%{domain}%",)
            )
        else:
            result = self.db_manager.execute_update("DELETE FROM cache_entries")

        return result.rowcount if result else 0

    def get_cache_statistics(self) -> dict:
        """Return cache usage statistics."""
        stats = self.db_manager.execute_query(
            "SELECT COUNT(*) as total_entries, "
            "SUM(LENGTH(response_data)) as total_size, "
            "COUNT(CASE WHEN datetime('now') > datetime(created_at, '+' || ttl_seconds || ' seconds') THEN 1 END) as expired_entries "
            "FROM cache_entries"
        )

        if stats:
            total_entries, total_size, expired_entries = stats[0]
            return {
                'total_entries': total_entries,
                'total_size_bytes': total_size or 0,
                'expired_entries': expired_entries,
                'hit_rate': self._calculate_hit_rate()
            }

        return {'total_entries': 0, 'total_size_bytes': 0, 'expired_entries': 0, 'hit_rate': 0.0}

    def _normalize_url(self, url: str) -> str:
        """Normalize URL for consistent cache keys."""
        from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

        parsed = urlparse(url)
        # Sort query parameters for consistency
        query_params = sorted(parse_qsl(parsed.query))
        normalized_query = urlencode(query_params)

        return urlunparse((
            parsed.scheme.lower(),
            parsed.netloc.lower(),
            parsed.path,
            parsed.params,
            normalized_query,
            ""  # Remove fragment
        ))

    def _normalize_request_body(self, body: bytes, content_type: str) -> str:
        """Normalize request body for cache key generation."""
        if not body:
            return ""

        if content_type and "application/json" in content_type:
            try:
                import json
                json_obj = json.loads(body.decode('utf-8'))
                return json.dumps(json_obj, sort_keys=True, separators=(',', ':'))
            except (json.JSONDecodeError, UnicodeDecodeError):
                pass

        # For non-JSON content, use hex representation
        return body.hex()
```

### 3.5 ThrottleManager

```python
class ThrottleManager:
    """Manages rate limiting and progressive throttling."""

    def __init__(self, throttling_config: dict):
        self.config = throttling_config
        self.default_requests_per_hour = throttling_config.get('default_requests_per_hour', 1000)
        self.domain_limits = throttling_config.get('domain_limits', {})
        self.progressive_enabled = throttling_config.get('progressive_enabled', True)

        # Request tracking
        self.request_counts = {}  # domain -> request timestamps
        self.throttle_state = {}  # domain -> throttle info

    def should_throttle(self, domain: str) -> bool:
        """Check if requests to domain should be throttled."""
        current_time = time.time()

        # Clean old request timestamps
        self._cleanup_old_requests(domain, current_time)

        # Get rate limit for domain
        rate_limit = self.domain_limits.get(domain, self.default_requests_per_hour)

        # Check current request count
        recent_requests = len(self.request_counts.get(domain, []))

        if recent_requests >= rate_limit:
            if self.progressive_enabled:
                self._apply_progressive_throttling(domain)
            return True

        return False

    def record_request(self, domain: str):
        """Record a request for throttling purposes."""
        current_time = time.time()

        if domain not in self.request_counts:
            self.request_counts[domain] = []

        self.request_counts[domain].append(current_time)

    def get_throttle_delay(self, domain: str) -> int:
        """Get the current throttle delay for a domain."""
        if domain in self.throttle_state:
            return self.throttle_state[domain]['delay_seconds']
        return 0

    def _cleanup_old_requests(self, domain: str, current_time: float):
        """Remove request timestamps older than 1 hour."""
        if domain not in self.request_counts:
            return

        hour_ago = current_time - 3600
        self.request_counts[domain] = [
            timestamp for timestamp in self.request_counts[domain]
            if timestamp > hour_ago
        ]

    def _apply_progressive_throttling(self, domain: str):
        """Apply progressive throttling based on consecutive violations."""
        if domain not in self.throttle_state:
            self.throttle_state[domain] = {
                'violations': 0,
                'delay_seconds': 1,
                'last_violation': time.time()
            }
        else:
            state = self.throttle_state[domain]
            state['violations'] += 1
            state['delay_seconds'] = min(state['delay_seconds'] * 2, 300)  # Max 5 minutes
            state['last_violation'] = time.time()
```

### 3.6 DatabaseManager

```python
class DatabaseManager:
    """Manages SQLite database operations with thread safety."""

    def __init__(self, database_path: str):
        self.database_path = database_path
        self.connection_pool = queue.Queue(maxsize=10)
        self._initialize_connections()

    def _initialize_connections(self):
        """Initialize connection pool."""
        import sqlite3

        for _ in range(5):  # Start with 5 connections
            conn = sqlite3.connect(
                self.database_path,
                check_same_thread=False,
                timeout=30.0
            )
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
            self.connection_pool.put(conn)

    def get_connection(self) -> sqlite3.Connection:
        """Get a connection from the pool."""
        try:
            return self.connection_pool.get(timeout=5.0)
        except queue.Empty:
            # Create new connection if pool is empty
            import sqlite3
            conn = sqlite3.connect(
                self.database_path,
                check_same_thread=False,
                timeout=30.0
            )
            return conn

    def return_connection(self, conn: sqlite3.Connection):
        """Return connection to pool."""
        try:
            self.connection_pool.put_nowait(conn)
        except queue.Full:
            conn.close()

    def execute_query(self, query: str, params: tuple = ()) -> list:
        """Execute a SELECT query and return results."""
        conn = self.get_connection()
        try:
            cursor = conn.execute(query, params)
            return cursor.fetchall()
        finally:
            self.return_connection(conn)

    def execute_update(self, query: str, params: tuple = ()) -> sqlite3.Cursor:
        """Execute an INSERT/UPDATE/DELETE query."""
        conn = self.get_connection()
        try:
            cursor = conn.execute(query, params)
            conn.commit()
            return cursor
        finally:
            self.return_connection(conn)
```

## 4. Configuration Management

### 4.1 Default Configuration

```python
DEFAULT_CONFIG = {
    "server": {
        "host": "127.0.0.1",
        "port": 8080,
        "max_workers": 10,
        "start_timeout": 5,
        "request_timeout": 30
    },
    "security": {
        "require_secure_key": False,
        "key_position": "path_prefix",
        "key_param_name": "proxy_key",
        "key_header_name": "X-Proxy-Key",
        "admin_rate_limit_per_minute": 10,
        "failed_auth_lockout_minutes": 5,
        "log_security_events": True
    },
    "cache": {
        "database_path": ":memory:",
        "default_ttl_days": 7,
        "max_entries": 10000,
        "max_size_mb": 500,
        "enable_compression": True,
        "cleanup_interval_hours": 24,
        "cacheable_methods": ["GET", "POST"],
        "cache_error_responses": False,
        "min_cache_response_size": 100,
        "max_cache_response_size": 10485760  # 10MB
    },
    "throttling": {
        "default_requests_per_hour": 1000,
        "domain_limits": {},
        "progressive_enabled": True,
        "progressive_max_delay": 300
    },
    "logging": {
        "level": "INFO",
        "include_secure_key": False,
        "log_to_parent": True,
        "parent_logger": None
    },
    "domain_mappings": {},
    "callbacks": {}
}
```

### 4.2 Configuration Validation

```python
class ConfigurationValidator:
    """Validates configuration and provides error reporting."""

    @staticmethod
    def validate_config(config: dict) -> tuple[bool, list[str]]:
        """Validate configuration and return validation results."""
        errors = []

        # Required fields
        if 'domain_mappings' not in config or not config['domain_mappings']:
            errors.append("Missing required 'domain_mappings' configuration")

        # Server configuration
        server_config = config.get('server', {})
        port = server_config.get('port', 8080)
        if not isinstance(port, int) or not 1024 <= port <= 65535:
            errors.append(f"Server port must be between 1024-65535, got {port}")

        host = server_config.get('host', '127.0.0.1')
        if not isinstance(host, str) or not host:
            errors.append("Server host must be a non-empty string")

        # Cache configuration
        cache_config = config.get('cache', {})
        ttl_days = cache_config.get('default_ttl_days', 7)
        if not isinstance(ttl_days, (int, float)) or ttl_days <= 0:
            errors.append(f"Cache TTL must be positive, got {ttl_days}")

        # Domain mappings validation
        domain_mappings = config.get('domain_mappings', {})
        for domain_key, mapping in domain_mappings.items():
            if not isinstance(mapping, dict):
                errors.append(f"Domain mapping for '{domain_key}' must be a dictionary")
                continue

            if 'upstream' not in mapping:
                errors.append(f"Domain mapping for '{domain_key}' missing 'upstream'")

        return len(errors) == 0, errors

    @staticmethod
    def merge_with_defaults(user_config: dict) -> dict:
        """Merge user configuration with defaults."""
        def deep_merge(default: dict, override: dict) -> dict:
            result = default.copy()
            for key, value in override.items():
                if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                    result[key] = deep_merge(result[key], value)
                else:
                    result[key] = value
            return result

        return deep_merge(DEFAULT_CONFIG, user_config)
```

## 5. Data Models

### 5.1 Database Schema

```sql
-- Cache entries table
CREATE TABLE IF NOT EXISTS cache_entries (
    cache_key TEXT PRIMARY KEY,
    response_data BLOB NOT NULL,
    headers TEXT NOT NULL,
    status_code INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ttl_seconds INTEGER NOT NULL,
    access_count INTEGER DEFAULT 0,
    last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Metrics table
CREATE TABLE IF NOT EXISTS metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    domain TEXT NOT NULL,
    method TEXT NOT NULL,
    cache_hit BOOLEAN NOT NULL,
    response_time_ms INTEGER,
    response_size_bytes INTEGER,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Configuration history (for audit)
CREATE TABLE IF NOT EXISTS config_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    config_data TEXT NOT NULL,
    changed_by TEXT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for performance
CREATE INDEX IF NOT EXISTS idx_cache_created_ttl ON cache_entries(created_at, ttl_seconds);
CREATE INDEX IF NOT EXISTS idx_metrics_domain_timestamp ON metrics(domain, timestamp);
CREATE INDEX IF NOT EXISTS idx_cache_last_accessed ON cache_entries(last_accessed);
```

### 5.2 Python Data Classes

```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
import json
from datetime import datetime

@dataclass
class CachedResponse:
    """Represents a cached HTTP response."""
    data: bytes
    headers: Dict[str, str]
    status_code: int
    created_at: str
    ttl_seconds: int
    access_count: int = 0
    last_accessed: Optional[str] = None

@dataclass
class ThrottleState:
    """Tracks throttling state for a domain."""
    violations: int = 0
    delay_seconds: int = 1
    last_violation: float = 0.0
    total_requests: int = 0

@dataclass
class RequestMetrics:
    """Metrics for a single request."""
    domain: str
    method: str
    cache_hit: bool
    response_time_ms: int
    response_size_bytes: int
    timestamp: datetime

@dataclass
class ProxyMetrics:
    """Overall proxy performance metrics."""
    total_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    total_response_time_ms: int = 0
    domains_served: int = 0
    uptime_seconds: float = 0.0

    @property
    def hit_rate(self) -> float:
        """Calculate cache hit rate."""
        total = self.cache_hits + self.cache_misses
        return (self.cache_hits / total) if total > 0 else 0.0

    @property
    def average_response_time(self) -> float:
        """Calculate average response time."""
        return (self.total_response_time_ms / self.total_requests) if self.total_requests > 0 else 0.0
```

## 6. Security Implementation

### 6.1 Cryptographic Security

```python
class CryptographicSecurity:
    """Handles all cryptographic operations."""

    @staticmethod
    def generate_secure_key() -> str:
        """Generate cryptographically secure random key."""
        import secrets
        import base64

        key_bytes = secrets.token_bytes(32)  # 256-bit key
        return base64.urlsafe_b64encode(key_bytes).decode('ascii').rstrip('=')

    @staticmethod
    def constant_time_compare(a: str, b: str) -> bool:
        """Constant-time string comparison to prevent timing attacks."""
        import hmac
        return hmac.compare_digest(a.encode('utf-8'), b.encode('utf-8'))

    @staticmethod
    def secure_hash(data: str) -> str:
        """Generate secure hash for cache keys."""
        import hashlib
        return hashlib.sha256(data.encode('utf-8')).hexdigest()
```

### 6.2 Rate Limiting

```python
class RateLimiter:
    """Token bucket rate limiter implementation."""

    def __init__(self, max_requests: int, time_window: int = 60):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = {}  # client_id -> request timestamps

    def is_allowed(self, client_id: str) -> bool:
        """Check if request is allowed within rate limits."""
        current_time = time.time()

        # Clean old requests
        if client_id in self.requests:
            cutoff_time = current_time - self.time_window
            self.requests[client_id] = [
                req_time for req_time in self.requests[client_id]
                if req_time > cutoff_time
            ]
        else:
            self.requests[client_id] = []

        # Check if under limit
        if len(self.requests[client_id]) < self.max_requests:
            self.requests[client_id].append(current_time)
            return True

        return False
```

## 7. Error Handling and Logging

### 7.1 Error Handling Strategy

```python
class ProxyError(Exception):
    """Base exception for proxy errors."""
    pass

class SecurityError(ProxyError):
    """Security-related errors."""
    pass

class CacheError(ProxyError):
    """Cache-related errors."""
    pass

class ThrottleError(ProxyError):
    """Throttling-related errors."""
    pass

class ConfigurationError(ProxyError):
    """Configuration-related errors."""
    pass

class ErrorHandler:
    """Centralized error handling."""

    def __init__(self, logger):
        self.logger = logger

    def handle_security_error(self, error: SecurityError, request_info: dict):
        """Handle security errors with appropriate logging."""
        self.logger.warning(
            f"Security error: {error}",
            extra={
                'client_ip': request_info.get('client_ip'),
                'user_agent': request_info.get('user_agent'),
                'request_path': request_info.get('path', '').split('/')[0:2]  # Don't log full path with key
            }
        )

    def handle_cache_error(self, error: CacheError, operation: str, cache_key: str):
        """Handle cache errors with recovery."""
        self.logger.error(
            f"Cache error during {operation}",
            extra={'operation': operation, 'error': str(error)}
        )

    def handle_upstream_error(self, error: Exception, url: str, method: str):
        """Handle upstream API errors."""
        self.logger.error(
            f"Upstream error: {method} {url}",
            extra={'method': method, 'url': url, 'error': str(error)}
        )
```

### 7.2 Logging Integration

```python
class LoggingAdapter:
    """Adapter for integrating with parent application logging."""

    def __init__(self, parent_logger=None, include_secure_key=False):
        self.include_secure_key = include_secure_key

        if parent_logger:
            self.logger = parent_logger.getChild('proxy')
        else:
            import logging
            self.logger = logging.getLogger('caching_proxy')

    def log_request(self, method: str, path: str, client_ip: str,
                   response_time: float, cache_hit: bool):
        """Log request with sanitized information."""
        # Sanitize path to remove secure key
        sanitized_path = self._sanitize_path(path)

        self.logger.info(
            f"{method} {sanitized_path} - {response_time:.2f}ms {'[CACHED]' if cache_hit else '[UPSTREAM]'}",
            extra={
                'method': method,
                'path': sanitized_path,
                'client_ip': client_ip,
                'response_time_ms': response_time,
                'cache_hit': cache_hit
            }
        )

    def _sanitize_path(self, path: str) -> str:
        """Remove secure key from path for logging."""
        if not self.include_secure_key and path.startswith('/'):
            parts = path.strip('/').split('/')
            if len(parts) >= 2:
                # Remove potential key (first part) and show generic placeholder
                return '/[KEY]/' + '/'.join(parts[1:])
        return path
```

## 8. Performance Optimization

### 8.1 Caching Strategy

- **Memory Tiering**: Use in-memory LRU cache for hot data, SQLite for persistence
- **Compression**: Gzip compression for responses > 1KB
- **Connection Pooling**: Reuse HTTP connections to upstream APIs
- **Background Cleanup**: Periodic cleanup of expired cache entries

### 8.2 Concurrency Model

```python
class ConcurrentServer:
    """Thread-based HTTP server for handling concurrent requests."""

    def __init__(self, host: str, port: int, max_workers: int):
        self.host = host
        self.port = port
        self.max_workers = max_workers
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)

    def start(self, handler_class):
        """Start the server with specified handler."""
        from http.server import ThreadingHTTPServer

        self.server = ThreadingHTTPServer((self.host, self.port), handler_class)
        self.server.daemon_threads = True
        self.server.timeout = 30

        # Start server in thread
        self.server_thread = threading.Thread(target=self.server.serve_forever)
        self.server_thread.daemon = True
        self.server_thread.start()
```

## 9. Testing Strategy

### 9.1 Unit Tests Structure

```
tests/
├── unit_tests/
│   ├── test_security_manager.py
│   ├── test_cache_engine.py
│   ├── test_throttle_manager.py
│   ├── test_configuration.py
│   └── test_url_transformation.py
├── integration_tests/
│   ├── test_proxy_integration.py
│   ├── test_cache_persistence.py
│   └── test_security_integration.py
└── performance_tests/
    ├── test_cache_performance.py
    ├── test_concurrent_requests.py
    └── test_memory_usage.py
```

### 9.2 Test Configuration

```python
TEST_CONFIG = {
    "server": {"host": "127.0.0.1", "port": 0},  # Random port
    "cache": {"database_path": ":memory:"},
    "security": {"require_secure_key": True},
    "domain_mappings": {
        "httpbin": {
            "upstream": "https://httpbin.org"
        }
    }
}
```

## 10. Deployment and Integration

### 10.1 Module Installation

```python
# setup.py
from setuptools import setup, find_packages

setup(
    name="reference-api-buddy",
    version="1.0.0",
    packages=find_packages(),
    install_requires=[
        # No external dependencies beyond Python standard library
    ],
    python_requires=">=3.8",
    author="Reference API Buddy Team",
    description="HTTP caching proxy module for development environments",
    long_description=open("README.md").read(),
    long_description_content_type="text/markdown",
)
```

### 10.2 Usage Examples

```python
# Basic usage
from api_buddy import CachingProxy

config = {
    "domain_mappings": {
        "openai": {
            "upstream": "https://api.openai.com"
        }
    }
}

with CachingProxy(config) as proxy:
    # Use proxy.get_secure_key() to configure your HTTP libraries
    pass

# Advanced usage with callbacks
def on_cache_hit(domain, url, response_time):
    print(f"Cache hit for {domain}: {response_time}ms")

config["callbacks"] = {"on_cache_hit": on_cache_hit}
proxy = CachingProxy(config)
```

This design provides a comprehensive technical blueprint for implementing the HTTP caching proxy module according to the requirements, with emphasis on security, performance, and ease of integration.
